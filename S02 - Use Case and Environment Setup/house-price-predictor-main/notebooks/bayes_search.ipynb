{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a21484b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "resume_bayes_history.py\n",
    "\n",
    "Flow:\n",
    "1) Load previous BayesSearchCV cv_results_ (or simulate).\n",
    "2) Define same skopt.space dimensions as original search.\n",
    "3) Convert cv_results_['params'] -> list-format aligned to dimensions.\n",
    "4) Instantiate skopt.Optimizer(n_initial_points=0).\n",
    "5) opt.tell(...) for every historical point (loss = -score if score is accuracy).\n",
    "6) Continue loop: opt.ask() -> evaluate via CV -> opt.tell()\n",
    "7) Refit final estimator on full data with best params found.\n",
    "\n",
    "Requirements:\n",
    "    pip install scikit-optimize scikit-learn numpy joblib\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import pickle, json, os\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from skopt import Optimizer\n",
    "from skopt.space import Integer, Real, Categorical\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9bccc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -------------------------\n",
    "# Helper: mapping utilities\n",
    "# -------------------------\n",
    "def dict_to_list_by_dims(pdict: Dict[str, Any], dims: List) -> List[Any]:\n",
    "    \"\"\"Convert param dict -> ordered list matching dims order.\n",
    "       Also apply encode mapping if dim is categorical or sentinel.\"\"\"\n",
    "    out = []\n",
    "    for d in dims:\n",
    "        name = d.name\n",
    "        val = pdict.get(name)\n",
    "        out.append(val)\n",
    "    return out\n",
    "\n",
    "def list_to_dict_by_dims(plist: List[Any], dims: List) -> Dict[str, Any]:\n",
    "    return {d.name: v for d, v in zip(dims, plist)}\n",
    "\n",
    "def load_cv_results(path: str) -> Dict[str, Any]:\n",
    "    \"\"\"Load cv_results from pickle or json.\"\"\"\n",
    "    if path.endswith('.pkl') or path.endswith('.pickle'):\n",
    "        with open(path, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    elif path.endswith('.json'):\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported format. Use .pkl or .json\")\n",
    "\n",
    "# -------------------------\n",
    "# Example: simulate prior cv_results_ (replace with real load)\n",
    "# -------------------------\n",
    "def simulate_prev_cv_results():\n",
    "    prev = {\n",
    "        'params': [\n",
    "            {'n_estimators': 50, 'max_depth': 5, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "            {'n_estimators': 100, 'max_depth': 8, 'min_samples_split': 2, 'criterion': 'gini'},\n",
    "            {'n_estimators': 30, 'max_depth': None, 'min_samples_split': 5, 'criterion': 'entropy'},\n",
    "            {'n_estimators': 200, 'max_depth': 12, 'min_samples_split': 3, 'criterion': 'gini'},\n",
    "        ],\n",
    "        # mean_test_score (higher is better)\n",
    "        'mean_test_score': [0.82, 0.84, 0.79, 0.85]\n",
    "    }\n",
    "    return prev\n",
    "\n",
    "# -------------------------\n",
    "# Main resume function\n",
    "# -------------------------\n",
    "def resume_bayes_search(cv_results_path: Optional[str] = None,\n",
    "                        n_new_iter: int = 20,\n",
    "                        random_state: int = 0):\n",
    "    \"\"\"\n",
    "    cv_results_path: optional path to previous BayesSearchCV.cv_results_ (pickle/json).\n",
    "                     If None, simulate a previous run.\n",
    "    n_new_iter: number of new evaluations to run after ingesting history.\n",
    "    \"\"\"\n",
    "    # 1) Define dimensions (MUST match the original search definition exactly)\n",
    "    dims = [\n",
    "        Integer(10, 300, name='n_estimators'),\n",
    "        # For max_depth we prefer using a Categorical that includes None if original allowed None:\n",
    "        Categorical([None] + list(range(1, 31)), name='max_depth'),\n",
    "        Integer(2, 10, name='min_samples_split'),\n",
    "        Categorical(['gini', 'entropy'], name='criterion'),\n",
    "    ]\n",
    "    param_names = [d.name for d in dims]\n",
    "    print(\"Parameter order:\", param_names)\n",
    "\n",
    "    # 2) Load previous cv_results_\n",
    "    if cv_results_path is None:\n",
    "        cv_results = simulate_prev_cv_results()\n",
    "        print(\"Using simulated previous cv_results_.\")\n",
    "    else:\n",
    "        cv_results = load_cv_results(cv_results_path)\n",
    "        print(f\"Loaded previous cv_results_ from {cv_results_path}\")\n",
    "\n",
    "    # Validate presence\n",
    "    prev_params = cv_results['params']\n",
    "    prev_scores = np.asarray(cv_results['mean_test_score'], dtype=float)\n",
    "    assert len(prev_params) == len(prev_scores), \"params and mean_test_score length mismatch\"\n",
    "\n",
    "    # 3) Convert prev params dict -> list in dims order\n",
    "    history: List[Tuple[List[Any], float]] = []\n",
    "    for p_dict, sc in zip(prev_params, prev_scores):\n",
    "        plist = dict_to_list_by_dims(p_dict, dims)\n",
    "        # Skopt expects plain Python types (not masked arrays)\n",
    "        plist = [int(x) if isinstance(x, (np.integer,)) else x for x in plist]\n",
    "        history.append((plist, float(sc)))\n",
    "\n",
    "    print(\"Extracted history length:\", len(history))\n",
    "    for p, s in history:\n",
    "        print(\"  \", p, \"->\", s)\n",
    "    print()\n",
    "\n",
    "    # 4) Instantiate Optimizer - set n_initial_points=0 to avoid extra random init\n",
    "    opt = Optimizer(\n",
    "        dimensions=dims,\n",
    "        base_estimator=\"GP\",\n",
    "        acq_func=\"EI\",\n",
    "        random_state=random_state,\n",
    "        n_initial_points=0\n",
    "    )\n",
    "\n",
    "    # 5) Tell optimizer about historical points (skopt minimizes; convert score->loss)\n",
    "    # If your scores are losses already, don't negate.\n",
    "    # Here we assume previous scores are accuracies (higher better) -> loss = -score\n",
    "    for plist, score in history:\n",
    "        loss = -score\n",
    "        opt.tell(plist, loss)\n",
    "    print(f\"Told optimizer {len(history)} historical points. Starting optimization...\")\n",
    "\n",
    "    # 6) Prepare evaluation function (CV)\n",
    "    X, y = make_classification(n_samples=2000, n_features=30, n_informative=10, random_state=0)\n",
    "    cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "    def evaluate(plist: List[Any]) -> float:\n",
    "        p_dict = list_to_dict_by_dims(plist, dims)\n",
    "        # convert types\n",
    "        n_estimators = int(p_dict['n_estimators'])\n",
    "        max_depth = p_dict['max_depth']  # possibly None\n",
    "        if isinstance(max_depth, np.integer):\n",
    "            max_depth = int(max_depth)\n",
    "        min_samples_split = int(p_dict['min_samples_split'])\n",
    "        criterion = p_dict['criterion']\n",
    "        clf = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            criterion=criterion,\n",
    "            random_state=0,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        # Use accuracy (higher better)\n",
    "        scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "        return float(np.mean(scores))\n",
    "\n",
    "    # 7) Find current best from history\n",
    "    hist_scores = [s for _, s in history]\n",
    "    best_score = max(hist_scores)\n",
    "    best_params_list = history[np.argmax(hist_scores)][0]\n",
    "    print(\"Best historical score:\", best_score, \"params:\", best_params_list)\n",
    "    print()\n",
    "\n",
    "    # 8) Continue loop: ask -> evaluate -> tell\n",
    "    for i in range(n_new_iter):\n",
    "        next_point = opt.ask()\n",
    "        score = evaluate(next_point)\n",
    "        opt.tell(next_point, -score)   # pass loss\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params_list = next_point\n",
    "        print(f\"[{i+1:02d}] tried {next_point} -> score {score:.4f} (best {best_score:.4f})\")\n",
    "\n",
    "    # 9) Final best and refit on full data\n",
    "    best_params = list_to_dict_by_dims(best_params_list, dims)\n",
    "    # convert types\n",
    "    best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "    if isinstance(best_params['max_depth'], np.integer):\n",
    "        best_params['max_depth'] = int(best_params['max_depth'])\n",
    "    best_params['min_samples_split'] = int(best_params['min_samples_split'])\n",
    "    print(\"\\nFinal best score:\", best_score)\n",
    "    print(\"Final best params:\", best_params)\n",
    "\n",
    "    final_clf = RandomForestClassifier(random_state=0, n_jobs=-1, **best_params)\n",
    "    final_clf.fit(X, y)\n",
    "    print(\"Refit final_clf on full X,y with best params. Done.\")\n",
    "\n",
    "    # Optionally: save optimizer state or history for later resume\n",
    "    result_obj = {\n",
    "        'best_score': best_score,\n",
    "        'best_params': best_params,\n",
    "        'optimizer_result': opt.get_result(),  # OptimizeResult\n",
    "        'history': history\n",
    "    }\n",
    "    # Note: opt.get_result() may not be pickle-friendly depending on base_estimator internals.\n",
    "    return result_obj\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c009c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    res = resume_bayes_search(cv_results_path=None, n_new_iter=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e5d17f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter order: ['n_estimators', 'max_depth', 'min_samples_split', 'criterion']\n"
     ]
    }
   ],
   "source": [
    "dims = [\n",
    "    Integer(10, 300, name='n_estimators'),\n",
    "    # For max_depth we prefer using a Categorical that includes None if original allowed None:\n",
    "    Categorical([None] + list(range(1, 31)), name='max_depth'),\n",
    "    Integer(2, 10, name='min_samples_split'),\n",
    "    Categorical(['gini', 'entropy'], name='criterion'),\n",
    "]\n",
    "param_names = [d.name for d in dims]\n",
    "print(\"Parameter order:\", param_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c438318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Integer(low=10, high=300, prior='uniform', transform='identity'),\n",
       " Categorical(categories=(None, 1, 2, ..., 28, 29, 30), prior=None),\n",
       " Integer(low=2, high=10, prior='uniform', transform='identity'),\n",
       " Categorical(categories=('gini', 'entropy'), prior=None)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7591bb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate presence\n",
    "prev_params = cv_results['params']\n",
    "prev_scores = np.asarray(cv_results['mean_test_score'], dtype=float)\n",
    "assert len(prev_params) == len(prev_scores), \"params and mean_test_score length mismatch\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dfeecb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted history length: 4\n",
      "   [50, 5, 2, 'gini'] -> 0.82\n",
      "   [100, 8, 2, 'gini'] -> 0.84\n",
      "   [30, None, 5, 'entropy'] -> 0.79\n",
      "   [200, 12, 3, 'gini'] -> 0.85\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3) Convert prev params dict -> list in dims order\n",
    "history: List[Tuple[List[Any], float]] = []\n",
    "for p_dict, sc in zip(prev_params, prev_scores):\n",
    "    plist = dict_to_list_by_dims(p_dict, dims)\n",
    "    # Skopt expects plain Python types (not masked arrays)\n",
    "    plist = [int(x) if isinstance(x, (np.integer,)) else x for x in plist]\n",
    "    history.append((plist, float(sc)))\n",
    "\n",
    "print(\"Extracted history length:\", len(history))\n",
    "for p, s in history:\n",
    "    print(\"  \", p, \"->\", s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33b75892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Told optimizer 4 historical points. Starting optimization...\n"
     ]
    }
   ],
   "source": [
    "n_new_iter: int = 20\n",
    "random_state: int = 0\n",
    "\n",
    "# 4) Instantiate Optimizer - set n_initial_points=0 to avoid extra random init\n",
    "opt = Optimizer(\n",
    "    dimensions=dims,\n",
    "    base_estimator=\"GP\",\n",
    "    acq_func=\"EI\",\n",
    "    random_state=random_state,\n",
    "    n_initial_points=0\n",
    ")\n",
    "\n",
    "# 5) Tell optimizer about historical points (skopt minimizes; convert score->loss)\n",
    "# If your scores are losses already, don't negate.\n",
    "# Here we assume previous scores are accuracies (higher better) -> loss = -score\n",
    "for plist, score in history:\n",
    "    loss = -score\n",
    "    opt.tell(plist, loss)\n",
    "print(f\"Told optimizer {len(history)} historical points. Starting optimization...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1272ff09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best historical score: 0.85 params: [200, 12, 3, 'gini']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6) Prepare evaluation function (CV)\n",
    "X, y = make_classification(n_samples=2000, n_features=30, n_informative=10, random_state=0)\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=0)\n",
    "\n",
    "def evaluate(plist: List[Any]) -> float:\n",
    "    p_dict = list_to_dict_by_dims(plist, dims)\n",
    "    # convert types\n",
    "    n_estimators = int(p_dict['n_estimators'])\n",
    "    max_depth = p_dict['max_depth']  # possibly None\n",
    "    if isinstance(max_depth, np.integer):\n",
    "        max_depth = int(max_depth)\n",
    "    min_samples_split = int(p_dict['min_samples_split'])\n",
    "    criterion = p_dict['criterion']\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_split=min_samples_split,\n",
    "        criterion=criterion,\n",
    "        random_state=0,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Use accuracy (higher better)\n",
    "    scores = cross_val_score(clf, X, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "# 7) Find current best from history\n",
    "hist_scores = [s for _, s in history]\n",
    "best_score = max(hist_scores)\n",
    "best_params_list = history[np.argmax(hist_scores)][0]\n",
    "print(\"Best historical score:\", best_score, \"params:\", best_params_list)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a953c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_new_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12bf7b8",
   "metadata": {},
   "source": [
    "Láº·p n_new_iter vÃ²ng: liÃªn tá»¥c há»i optimizer nÃªn thá»­ cáº¥u hÃ¬nh nÃ o (opt.ask()), train + cross-validate Ä‘á»ƒ láº¥y score, bÃ¡o káº¿t quáº£ láº¡i (opt.tell()).\n",
    "\n",
    "TiÃªu chÃ­ chá»n: score tá»‘t nháº¥t â†’ update best.\n",
    "\n",
    "In progress Ä‘á»ƒ quan sÃ¡t live quÃ¡ trÃ¬nh search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09ed124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01] tried [np.int64(300), np.int64(12), np.int64(10), np.str_('gini')] -> score 0.8755 (best 0.8755)\n",
      "[02] tried [np.int64(300), np.int64(12), np.int64(3), np.str_('entropy')] -> score 0.8735 (best 0.8755)\n",
      "[03] tried [np.int64(272), np.int64(8), np.int64(5), np.str_('gini')] -> score 0.8690 (best 0.8755)\n",
      "[04] tried [np.int64(300), np.int64(19), np.int64(8), np.str_('gini')] -> score 0.8800 (best 0.8800)\n",
      "[05] tried [np.int64(300), np.int64(8), np.int64(10), np.str_('gini')] -> score 0.8645 (best 0.8800)\n",
      "[06] tried [np.int64(300), np.int64(14), np.int64(7), np.str_('gini')] -> score 0.8785 (best 0.8800)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngolong/Documents/CODE_BASED/DevOps-to-MLOps-Bootcamp-Build-Deploy-ML-Systems-End-to-End/venv_mlops/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(300), np.int64(19), np.int64(8), np.str_('gini')] before, using random point [np.int64(290), 18, np.int64(6), 'gini']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[07] tried [np.int64(290), 18, np.int64(6), 'gini'] -> score 0.8820 (best 0.8820)\n",
      "[08] tried [np.int64(287), np.int64(30), np.int64(10), np.str_('gini')] -> score 0.8775 (best 0.8820)\n",
      "[09] tried [np.int64(300), np.int64(6), np.int64(10), np.str_('gini')] -> score 0.8415 (best 0.8820)\n",
      "[10] tried [np.int64(300), np.int64(2), np.int64(9), np.str_('gini')] -> score 0.7625 (best 0.8820)\n",
      "[11] tried [np.int64(300), np.int64(18), np.int64(9), np.str_('gini')] -> score 0.8785 (best 0.8820)\n",
      "[12] tried [np.int64(279), np.int64(19), np.int64(9), np.str_('gini')] -> score 0.8785 (best 0.8820)\n",
      "[13] tried [np.int64(300), np.int64(19), np.int64(2), np.str_('gini')] -> score 0.8775 (best 0.8820)\n",
      "[14] tried [np.int64(300), np.int64(17), np.int64(9), np.str_('gini')] -> score 0.8780 (best 0.8820)\n",
      "[15] tried [np.int64(300), np.int64(1), np.int64(4), np.str_('gini')] -> score 0.7435 (best 0.8820)\n",
      "[16] tried [np.int64(300), np.int64(18), np.int64(10), np.str_('gini')] -> score 0.8755 (best 0.8820)\n",
      "[17] tried [np.int64(300), np.int64(18), np.int64(2), np.str_('gini')] -> score 0.8785 (best 0.8820)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngolong/Documents/CODE_BASED/DevOps-to-MLOps-Bootcamp-Build-Deploy-ML-Systems-End-to-End/venv_mlops/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(300), np.int64(18), np.int64(2), np.str_('gini')] before, using random point [np.int64(219), 2, np.int64(2), 'entropy']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18] tried [np.int64(219), 2, np.int64(2), 'entropy'] -> score 0.7555 (best 0.8820)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ngolong/Documents/CODE_BASED/DevOps-to-MLOps-Bootcamp-Build-Deploy-ML-Systems-End-to-End/venv_mlops/lib/python3.11/site-packages/skopt/optimizer/optimizer.py:517: UserWarning: The objective has been evaluated at point [np.int64(300), np.int64(18), np.int64(2), np.str_('gini')] before, using random point [np.int64(80), 16, np.int64(4), 'entropy']\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19] tried [np.int64(80), 16, np.int64(4), 'entropy'] -> score 0.8690 (best 0.8820)\n",
      "[20] tried [np.int64(300), np.int64(16), np.int64(9), np.str_('gini')] -> score 0.8805 (best 0.8820)\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_new_iter):\n",
    "    next_point = opt.ask()                # ðŸ§­ Query optimizer cho Ä‘iá»ƒm má»›i (tham sá»‘ hyperparam má»›i)\n",
    "    score = evaluate(next_point)          # ðŸ§ª ÄÃ¡nh giÃ¡ Ä‘iá»ƒm (train model, láº¥y score CV)\n",
    "    opt.tell(next_point, -score)          # ðŸ“© Ghi láº¡i Ä‘iá»ƒm cho optimizer (chuyá»ƒn score thÃ nh loss, do skopt minimize)\n",
    "    if score > best_score:                # ðŸ† Update náº¿u tá»‘t hÆ¡n best hiá»‡n táº¡i\n",
    "        best_score = score\n",
    "        best_params_list = next_point\n",
    "    print(f\"[{i+1:02d}] tried {next_point} -> score {score:.4f} (best {best_score:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a1a2da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final best score: 0.8819981900941422\n",
      "Final best params: {'n_estimators': 290, 'max_depth': 18, 'min_samples_split': 6, 'criterion': 'gini'}\n",
      "Refit final_clf on full X,y with best params. Done.\n",
      "result ----------------------------------------------------------------------------------------------------\n",
      "{'best_score': 0.8819981900941422, 'best_params': {'n_estimators': 290, 'max_depth': 18, 'min_samples_split': 6, 'criterion': 'gini'}, 'optimizer_result':           fun: -0.8819981900941422\n",
      "            x: [np.int64(290), 18, np.int64(6), 'gini']\n",
      "    func_vals: [-8.200e-01 -8.400e-01 ... -8.690e-01 -8.805e-01]\n",
      "      x_iters: [[50, 5, 2, 'gini'], [100, 8, 2, 'gini'], [30, None, 5, 'entropy'], [200, 12, 3, 'gini'], [np.int64(300), np.int64(12), np.int64(10), np.str_('gini')], [np.int64(300), np.int64(12), np.int64(3), np.str_('entropy')], [np.int64(272), np.int64(8), np.int64(5), np.str_('gini')], [np.int64(300), np.int64(19), np.int64(8), np.str_('gini')], [np.int64(300), np.int64(8), np.int64(10), np.str_('gini')], [np.int64(300), np.int64(14), np.int64(7), np.str_('gini')], [np.int64(290), 18, np.int64(6), 'gini'], [np.int64(287), np.int64(30), np.int64(10), np.str_('gini')], [np.int64(300), np.int64(6), np.int64(10), np.str_('gini')], [np.int64(300), np.int64(2), np.int64(9), np.str_('gini')], [np.int64(300), np.int64(18), np.int64(9), np.str_('gini')], [np.int64(279), np.int64(19), np.int64(9), np.str_('gini')], [np.int64(300), np.int64(19), np.int64(2), np.str_('gini')], [np.int64(300), np.int64(17), np.int64(9), np.str_('gini')], [np.int64(300), np.int64(1), np.int64(4), np.str_('gini')], [np.int64(300), np.int64(18), np.int64(10), np.str_('gini')], [np.int64(300), np.int64(18), np.int64(2), np.str_('gini')], [np.int64(219), 2, np.int64(2), 'entropy'], [np.int64(80), 16, np.int64(4), 'entropy'], [np.int64(300), np.int64(16), np.int64(9), np.str_('gini')]]\n",
      "       models: [GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396), GaussianProcessRegressor(kernel=1**2 * Matern(length_scale=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], nu=2.5) + WhiteKernel(noise_level=1),\n",
      "                                        n_restarts_optimizer=2, noise='gaussian',\n",
      "                                        normalize_y=True, random_state=209652396)]\n",
      "        space: Space([Integer(low=10, high=300, prior='uniform', transform='normalize'),\n",
      "                      Categorical(categories=(None, 1, 2, ..., 28, 29, 30), prior=None),\n",
      "                      Integer(low=2, high=10, prior='uniform', transform='normalize'),\n",
      "                      Categorical(categories=('gini', 'entropy'), prior=None)])\n",
      " random_state: RandomState(MT19937)\n",
      "        specs:     args:              dimensions: [Integer(low=10, high=300, prior='uniform', transform='normalize'), Categorical(categories=(None, 1, 2, ..., 28, 29, 30), prior=None), Integer(low=2, high=10, prior='uniform', transform='normalize'), Categorical(categories=('gini', 'entropy'), prior=None)]\n",
      "                                  base_estimator: GP\n",
      "                                 n_random_starts: None\n",
      "                                n_initial_points: 0\n",
      "                         initial_point_generator: random\n",
      "                                          n_jobs: 1\n",
      "                                        acq_func: EI\n",
      "                                   acq_optimizer: auto\n",
      "                                    random_state: 0\n",
      "                                model_queue_size: None\n",
      "                                space_constraint: None\n",
      "                                 acq_func_kwargs: None\n",
      "                            acq_optimizer_kwargs: None\n",
      "                                avoid_duplicates: True\n",
      "               function: Optimizer, 'history': [([50, 5, 2, 'gini'], 0.82), ([100, 8, 2, 'gini'], 0.84), ([30, None, 5, 'entropy'], 0.79), ([200, 12, 3, 'gini'], 0.85)]}\n"
     ]
    }
   ],
   "source": [
    "# 9) Final best and refit on full data\n",
    "best_params = list_to_dict_by_dims(best_params_list, dims)\n",
    "\n",
    "# convert types\n",
    "best_params['n_estimators'] = int(best_params['n_estimators'])\n",
    "if isinstance(best_params['max_depth'], np.integer):\n",
    "    best_params['max_depth'] = int(best_params['max_depth'])\n",
    "best_params['min_samples_split'] = int(best_params['min_samples_split'])\n",
    "print(\"\\nFinal best score:\", best_score)\n",
    "print(\"Final best params:\", best_params)\n",
    "\n",
    "final_clf = RandomForestClassifier(random_state=0, n_jobs=-1, **best_params)\n",
    "final_clf.fit(X, y)\n",
    "print(\"Refit final_clf on full X,y with best params. Done.\")\n",
    "\n",
    "# Optionally: save optimizer state or history for later resume\n",
    "result_obj = {\n",
    "    'best_score': best_score,\n",
    "    'best_params': best_params,\n",
    "    'optimizer_result': opt.get_result(),  # OptimizeResult\n",
    "    'history': history\n",
    "}\n",
    "\n",
    "print(\"result\" + \" \" + \"-\"*100)\n",
    "print(result_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e54b407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
